if (length(y) != dim(x)[1]) {
stop(paste("number of observations of y (", length(y), ") not equal to the number of rows of x (", dim(x)[1], ").", sep = ""))
}
# checks if the quantile level is one-dimensional
if (length(tau) > 1) {
stop(paste("quantile level needs to be one number."))
}
# checks if the quantile level is between 0 and 1 (strictly)
if (tau >= 1 | tau <= 0) {
stop(paste("quantile level needs to be a number strictly between 0 and 1."))
}
# checks for NAs
if (sum(is.na(y)) > 0 | sum(is.na(x)) > 0) {
stop(paste("Data include missing values."))
}
# checks if n>p
if (length(y) <= dim(x)[2]) {
stop(paste("number of observations of y (", length(y), ") should be greater than the number of columns of x (", dim(x)[2], ").", sep = ""))
}
if (length(x) == length(y)) {
stop("x is one dimensional, no need for dimension reduction.")
}
# define the parameters
n <- length(y)
p <- dim(x)[2]
# standardize the predictor variables
xc <- scale(x, scale = FALSE)
sig <- var(x)
signrt <- MTS::msqrt(sig)$invsqrt
xstand <- xc %*% signrt
# use SIR for initial dimension reduction
# use bic_d to estimate d, the dimension of the central subspace
output <- dr::dr(y ~ xstand)
lambdas <- output$evalues
d_hat <- bic_d(lambdas, n)
ahat <- cbind(output$evectors[, 1:d_hat])
newx <- xstand %*% ahat
d <- d_hat
# define the bandwidth and estimate the conditional quantile
red_dim <- floor(0.2 * n) # find how many observations correspond to a 20%
index_y <- order(y)[red_dim:(n - red_dim)] # subtract the smallest 20% and the largest 20% of the observations
h <- KernSmooth::dpill(newx[index_y, ], y[index_y])
h <- h * (tau * (1 - tau) / (dnorm(qnorm(tau)))^2)^.2
if (h == 'NaN') {
h <- 1.25 * max(n^(-1 / (d + 4)), min(2, sd(y)) * n^(- 1 / (d + 4)))
} else {
h <- 1.25 * max(n^(-1 / (d + 4)), min(2, sd(y)) * n^(- 1 / (d + 4)), h) # maximum of all bandwidths
}
non_par <- llqr(newx, y, tau = tau, h = h)
qhat <- non_par$ll_est
# define the initial vector, i.e., the ordinary least squares estimator from
# regressing qhat on x
beta_hat <- (solve(crossprod(cbind(1, xstand))) %*% crossprod(cbind(1, xstand), qhat))[-1]
# if dtau is missing, use the iterative procedure to produce all vectors
# apply the BIC criterion to determine dtau
if (is.null(dtau)) {
b <- matrix(0, p, p)
b[, 1] <- beta_hat
for (j in 2:(dim(x)[2])) {
newx <- xstand %*% b[, j - 1]
hatq <- llqr(newx, y, tau = tau, h = h)$ll_est
mat <- matrix(0, dim(x)[1], dim(x)[2])
for (i in 1:(dim(x)[1])) {
mat[i, ] <- hatq[i] * xstand[i, ]
}
b[, j] <- apply(mat, 2, mean)
}
B <- tcrossprod(b, b)
eigenvalues <- eigen(B)$values
out <- eigen(B)$vectors
out <- signrt %*% out
dtau <- bic_d(eigenvalues, n)
list(qvectors = out, qvalues = eigenvalues, dtau = dtau)
} else if (dtau > 1) {
# if dtau is known to be greater than 1, then use the iterative procedure to
# produce more vectors
# check that dtau is integer
if (dtau<1 | dtau != as.integer(dtau) | dtau > p){
stop(paste("dtau needs to be an integer between 1 and p (", dim(x)[2], ")."))
}
b <- matrix(0, p, p)
b[, 1] <- beta_hat
for (j in 2:(dim(x)[2])) {
newx <- xstand %*% b[, j - 1]
hatq <- llqr(newx, y, tau = tau, h = h)$ll_est
mat <- matrix(0, dim(x)[1], dim(x)[2])
for (i in 1:(dim(x)[1])) {
mat[i, ] <- hatq[i] * xstand[i, ]
}
b[, j] <- apply(mat, 2, mean)
}
B <- tcrossprod(b, b)
eigenvalues <- eigen(B)$values
out <- eigen(B)$vectors
out <- signrt %*% out
list(qvectors = out, qvalues = eigenvalues, dtau = dtau)
} else {
# check that dtau is integer
if (dtau<1 | dtau != as.integer(dtau) | dtau > p){
stop(paste("dtau needs to be an integer between 1 and p (", dim(x)[2], ")."))
}
# if dtau is known to be one, then the initial vector is sufficient
out <- signrt %*% beta_hat
out <- out / sqrt(sum(out^2))
dtau <- dtau
list(qvectors = out, dtau = dtau)
}
}
llqr <- function(x, y, tau=0.5, h = NULL, method="rule", x0 = NULL) {
x <- as.matrix(x)
y <- as.matrix(y)
# compatibility checks
# checks if y is univariate
if (dim(y)[2] > 1) {
stop(paste("y needs to be a univariate response. y is a", dim(y)[2], "-dimensional response in this case."))
}
# checks if the number of observations for x and y agree
if (length(y) != dim(x)[1]) {
stop(paste("number of observations of y (", length(y), ") not equal to the number of rows of x (", dim(x)[1], ").", sep = ""))
}
# checks if the quantile level is one-dimensional
if (length(tau) > 1) {
stop(paste("quantile level needs to be one number."))
}
# checks if the quantile level is between 0 and 1 (strictly)
if (tau >= 1 | tau <= 0) {
stop(paste("quantile level needs to be a number strictly between 0 and 1."))
}
# checks for NAs
if (sum(is.na(y)) > 0 | sum(is.na(x)) > 0) {
stop(paste("Data include missing values."))
}
# checks if n>p
if (length(y) <= dim(x)[2]) {
stop(paste("number of observations of y (", length(y), ") should be greater than the number of columns of x (", dim(x)[2], ").", sep = ""))
}
n <- length(y)
p <- dim(x)[2]
# if bandwidth is missing, estimate it
if (is.null(h)) {
if (method == "CV") {
h <- llqrcv(x, y, tau)
}
if (method == "rule") {
red_dim <- floor(0.2 * n) # find how many observations correspond to a 20%
index_y <- order(y)[red_dim:(n - red_dim)] # subtract the smallest 20% and the largest 20% of the observations
h <- KernSmooth::dpill(x[index_y, ], y[index_y])
h <- 1.25 * h * (tau * (1 - tau) / (dnorm(qnorm(tau)))^2)^.2
if (h == 'NaN') {
h <- 1.25 * max(n^(-1 / (p + 4)), min(2, sd(y)) * n^(- 1 / (p + 4)))
}
}
} else {
h <- h
}
# if x0 is missing, perform estimation at the entire design matrix
if (is.null(x0)) {
ll_est <- list()
# if the dimension of x is one, use univariate kernel, otherwise
# use multivariate kernel
if (p == 1) {
# perform estimation at the design matrix x
for (i in 1:dim(x)[1]) {
z <- x - x[i, 1]
w <- dnorm(z / h)
q <- quantreg::rq(y ~ z, weights = w, tau = tau, ci = FALSE)
ll_est[i] <- q$coef[1]
}
} else {
for (i in 1:dim(x)[1]) {
z <- list()
z <- t(t(x) - x[i, ])
w <- mvtnorm::dmvnorm(z / h)
q <- quantreg::rq(y ~ z, weights = w, tau = tau, ci = FALSE)
ll_est[i] <- q$coef[1]
}
}
} else {
# checks if the dimension of x0 is the same as p
if (length(x0) != p) {
stop(paste("x0 needs to be a p-dimensional vector, where p is the dimension of x (",dim(x)[2], ")."))
}
if (p == 1) {
# perform estimation at the point x0
z <- x - x0
w <- dnorm(z / h)
q <- quantreg::rq(y ~ z, weights = w, tau = tau, ci = FALSE)
ll_est <- q$coef[1]
} else {
z <- t(t(x) - x0)
w <- mvtnorm::dmvnorm(z / h)
q <- quantreg::rq(y ~ z, weights = w, tau = tau, ci = FALSE)
ll_est <- q$coef[1]
}
}
list(ll_est = unlist(ll_est), h = h)
}
ValAR <- function(y, p, tau, movwind = NULL, chronological = TRUE){
# compatibility checks
# check whether y is a vector
if (is.vector(y) == FALSE) {
stop(paste("y needs to be a vector."))
}
# checks for NAs
if (sum(is.na(y)) > 0) {
stop(paste("Data include missing values."))
}
#  checks if n >  p
if (length(y) <= p){
stop(paste("number of observations of y (", length(y), ") should be  greater than p."))
}
# checks if the quantile level is one-dimensional
if (length(tau) > 1) {
stop(paste("quantile level needs to be one number."))
}
# checks if the quantile level is between 0 and 1 (strictly)
if (tau >= 1 | tau <= 0) {
stop(paste("quantile level needs to be a number strictly between 0 and 1."))
}
# checks if p is an integer
if (p != round(p)) {
stop(paste("p needs to be an integer."))
}
# check that the returns is an increasing order, i.e., from past to present
# if not, reverse the vector of returns
if (chronological == FALSE) {
y <- rev(y)
}
# delete the last p observations, since the full data can start from the p + 1
# observation. This will give the new vector of returns where a matrix X will exist
newy <- y[-c(1:p)]
n <- length(newy)
# define the design matrix X, which is defined as the previous p observations
# for each return
X <- matrix(0, n, p)
for (i in (p + 1):(n + p)){
X[i - p, ] <- y[(i - p):(i - 1)]
}
# If a moving window is provided, define the new reduced vector of returns
# and design matrix X, i.e., take the last movwind observations.
if (is.null(movwind) == FALSE) {
# first, checks
# checks if moving window is an integer
if (movwind != round(movwind)) {
stop(paste('Moving window needs to be an integer.'))
}
# checks if moving window is a number between p and n
if (movwind > (length(y) - p) | movwind < p){
stop(paste("Moving window needs to be greater than the number of covariates
(", p, ") and less than the number of used observations n - p (", length(y) - p, ")"))
}
newy <- newy[(n - movwind + 1):n]
X <- X[(n - movwind + 1):n, ]
n <- length(newy)
} else {
movwind <- min(250, length(newy))
newy <- newy[(n - movwind + 1):n]
X <- X[(n - movwind + 1):n, ]
n <- length(newy)
}
# one-step ahead prediction
out <- cqs(x = X, y = newy, tau = tau)
beta_hat <- as.matrix(out$qvectors[, 1:out$dtau])
newx = X %*% beta_hat
x0 = as.vector(newy[(n - p + 1):n] %*% beta_hat)
qhat_est <- as.numeric(llqr(newx, newy, tau, x0 = x0)$ll_est)
return(VaR = qhat_est)
}
bic_d <- function(lambdas, n) {
lambdas <- sort(lambdas, decreasing = TRUE)
p <- length(lambdas)
gn <- as.null(p)
for (i in 1:length(lambdas)) {
gn[i] <- n * sum((lambdas[1:i])^2) / sum((lambdas)^2) -
2 * (n^ (3 / 4) / p) * i * (i + 1) / 2
}
which(gn == max(gn))
}
library(PerformanceAnalytics)
data(edhec, package = "PerformanceAnalytics")
head(edhec)
y <- as.vector(edhec[, 2])
n <- length(y)
p <- 5
tau <- 0.05
ValAR(y, p = p, tau = tau)
y <- as.vector(edhec[, 2])
n <- length(y)
p <- 5
tau <- 0.05
ValAR(y, p = p, tau = tau)
VaR(y, 0.95, method = 'historical')
size <- 100
VaRest <- as.null(size)
for (i in 1:size){
VaRest[i] <- ValAR(y[1:(n - size + i - 1)], p, tau)
}
plot.ts(y[(n - size + 1):n], ylim = range(y[(n - size + 1):n], VaRest), ylab = 'returns')
lines(VaRest, col = 'red')
sum(y[(n - size + 1):n] < VaRest) / size
taus <- c(0.01, 0.025, 0.05)
VaRest <- matrix(0, size, length(taus))
for (i in 1:size) {
for (j in 1:length(taus)) {
VaRest[i, j] <- ValAR(y[1:(n - size + i - 1)], p, taus[j])
}
}
# plots
plot.ts(y[(n - size + 1):n], ylim = range(y[(n - size + 1):n], VaRest), ylab = 'returns')
lines(VaRest[, 1], col = 'red')
lines(VaRest[, 2], col = 'blue')
lines(VaRest[, 3], col = 'green')
legend('top', legend = c("1%", "2.5%", "5%"), col = c("red", "blue", "green"),
lty=1, cex=1, horiz = T, bty = "n")
# proportion of exceptions
sum(y[(n - size + 1):n] < VaRest[, 1]) / size
sum(y[(n - size + 1):n] < VaRest[, 2]) / size
sum(y[(n - size + 1):n] < VaRest[, 3]) / size
devtools::load_all(".")
library(quantdr)
library(PerformanceAnalytics)
data(edhec, package = "PerformanceAnalytics")
head(edhec)
y <- as.vector(edhec[, 2])
n <- length(y)
p <- 5
tau <- 0.05
ValAR(y, p = p, tau = tau)
VaR(y, 0.95, method = 'historical')
size <- 100
VaRest <- as.null(size)
for (i in 1:size){
VaRest[i] <- ValAR(y[1:(n - size + i - 1)], p, tau)
}
plot.ts(y[(n - size + 1):n], ylim = range(y[(n - size + 1):n], VaRest), ylab = 'returns')
lines(VaRest, col = 'red')
taus <- c(0.01, 0.025, 0.05)
VaRest <- matrix(0, size, length(taus))
for (i in 1:size) {
for (j in 1:length(taus)) {
VaRest[i, j] <- ValAR(y[1:(n - size + i - 1)], p, taus[j])
}
}
# plots
plot.ts(y[(n - size + 1):n], ylim = range(y[(n - size + 1):n], VaRest), ylab = 'returns')
lines(VaRest[, 1], col = 'red')
lines(VaRest[, 2], col = 'blue')
lines(VaRest[, 3], col = 'green')
legend('top', legend = c("1%", "2.5%", "5%"), col = c("red", "blue", "green"),
lty=1, cex=1, horiz = T, bty = "n")
# proportion of exceptions
sum(y[(n - size + 1):n] < VaRest[, 1]) / size
sum(y[(n - size + 1):n] < VaRest[, 2]) / size
sum(y[(n - size + 1):n] < VaRest[, 3]) / size
library(PerformanceAnalytics)
data(edhec, package = "PerformanceAnalytics")
head(edhec)
y <- as.vector(edhec[, 2])
n <- length(y)
p <- 5
tau <- 0.05
ValAR(y, p = p, tau = tau)
VaR(y, 0.95, method = 'historical')
library(PerformanceAnalytics)
data(edhec, package = "PerformanceAnalytics")
head(edhec)
y <- as.vector(edhec[, 2])
n <- length(y)
p <- 5
tau <- 0.05
ValAR(y, p = p, tau = tau)
VaR(y, 0.95, method = 'historical')
taus <- c(0.01, 0.025, 0.05)
VaRest <- matrix(0, size, length(taus))
for (i in 1:size) {
for (j in 1:length(taus)) {
VaRest[i, j] <- ValAR(y[1:(n - size + i - 1)], p, taus[j])
}
}
# plots
plot.ts(y[(n - size + 1):n], ylim = range(y[(n - size + 1):n], VaRest), ylab = 'returns')
lines(VaRest[, 1], col = 'red')
lines(VaRest[, 2], col = 'blue')
lines(VaRest[, 3], col = 'green')
legend('top', legend = c("1%", "2.5%", "5%"), col = c("red", "blue", "green"),
lty=1, cex=1, horiz = T, bty = "n")
plot.ts(y[(n - size + 1):n], ylim = range(y[(n - size + 1):n], VaRest), ylab = 'returns')
lines(VaRest[, 1], col = 'red')
lines(VaRest[, 2], col = 'blue')
lines(VaRest[, 3], col = 'green')
legend('top', legend = c("1%", "2.5%", "5%"), col = c("red", "blue", "green"),
lty=1, cex=1, horiz = T, bty = "n")
devtools::load_all(".")
devtools::check_win_devel()
devtools::check_rhub()
devtools::load_all(".")
devtools::check_win_devel()
devtools::check_rhub()
devtools::release()
devtools::release()
devtools::load_all(".")
library(quantdr)
library(quantdr)
# install.packages("devtools")
devtools::install_github("elianachristou/quantdr")
library(quantdr)
library(quantdr)
devtools::install_github("elianachristou/quantdr")
library(quantdr)
devtools::install_github("elianachristou/quantdr")
devtools::load_all(".")
devtools::load_all()
usethis::use_testthat()
devtools::test()
devtools::check_win_devel()
devtools::revdep_check()
devtools::check_rhub()
devtools::check_rhub()
# install.packages("devtools")
devtools::install_github("elianachristou/quantdr")
library(quantdr)
## basic example code - a homoscedastic single-index model
# Setting
set.seed(1234)
n <- 100
p <- 10
taus <- c(0.1, 0.25, 0.5, 0.75, 0.9)
x <- matrix(rnorm(n * p), n, p)
error <- rnorm(n)
y <- 3 * x[, 1] + x[, 2] + error
# true direction that spans each central quantile subspace
beta_true <- c(3, 1, rep(0, p - 2))
beta_true / sqrt(sum(beta_true^2))
# sufficient direction
dir1 <- x %*% beta_true
# Estimate the directions of each central quantile subspace
# Since dtau is known to be one, the algorithm will produce only one vector
out <- matrix(0, p, length(taus))
for (i in 1:length(taus)) {
out[, i] <- cqs(x, y, tau = taus[i], dtau = 1)$qvectors
}
out
# compare each estimated direction with the true one using the angle between the two subspaces
library(pracma)
for (i in 1:length(taus)) {
print(subspace(out[, i], beta_true) / (pi / 2)) # the angle is measured in radians, so divide by pi/2
}
# Estimate and plot the conditional quantile function using the new sufficient predictors
library(ggplot2)
newx <- x %*% out
qhat <- as.null()
for (i in 1:length(taus)) {
qhat <- c(qhat, llqr(newx[, i], y, tau = taus[i])$ll_est)
}
data1 <- data.frame(rep(dir1, n), rep(y, n), c(newx), rep(taus, each = n), qhat)
names(data1) <- c("dir1", "y", "newx", "quantiles", 'qhat')
ggplot(data1, aes(x = dir1, y = y)) + geom_point(size = 1) +
geom_point(aes(x = dir1, qhat), colour = 'red', size = 1) +
facet_wrap(~quantiles, ncol = 3) + xlab('sufficient direction')
library(MASS)
attach(Boston)
# read the data
y <- medv
x <- cbind(rm, log(tax), ptratio, log(lstat))
n <- length(y)
p <- dim(x)[2]
# plot the estimated coefficient of each predictor variable for multiple quantiles
tau <- seq(0.1, 0.9, by = 0.005)
beta_hat <- matrix(0, p, length(tau))
for (k in 1:length(tau)) {
out <- cqs(x, y, tau = tau[k])
beta_hat[, k] <- out$qvectors[, 1:out$dtau] # the suggested dimension of the central quantile subspace is 1
}
devtools::build()
devtools::release()
devtools::release()
check_win_devel()
devtools::check_win_devel()
devtools::check_rhub()
install.packages("rlang", dependencies = c("Depends", "Imports", "LinkingTo", "Suggests"))
library(quantdr)
install.packages("rlang", dependencies = c("Depends", "Imports", "LinkingTo", "Suggests"))
library(quantdr)
library(quantdr)
devtools::check_win_devel()
devtools::revdep_check()
devtools::check_rhub()
devtools::build()
devtools::release()
seq(0, 0.05, length = 10)
?rq
library(quantreg)
?Rq
?rq
devtools::load_all(".")
library(quantdr)
devtools::build()
devtools::release()
devtools::build()
devtools::release()
16107/921
library(quantdr)
install.packages('quantdr')
library(quantdr)
cqs
bic_d
devtools::load_all()
?cqs
?stats
library(help = "stats")
?mean
usethis::use_roxygen_md()
