p <- 5
tau <- 0.05
ValAR(y, p = p, tau = tau)
i
size <- 100
VaRest <- as.null(size)
for (i in 1:size){
VaRest[i] <- ValAR(y[1:(n - size + i - 1)], p, tau)
}
i
VaRest[i] <- ValAR(y[1:(n - size + i - 1)], p, tau, movwind = 200)
VaRest[i] <- ValAR(y[1:(n - size + i - 1)], p, tau, movwind = 180)
VaRest[i] <- ValAR(y[1:(n - size + i - 1)], p, tau, movwind = 188)
y <- as.vector(edhec[, 2])
n <- length(y)
p <- 5
tau <- 0.05
ValAR(y, p = p, tau = tau)
VaR(y, 0.95, method = 'historical')
size <- 100
VaRest <- as.null(size)
for (i in 1:size){
VaRest[i] <- ValAR(y[1:(n - size + i - 1)], p, tau)
}
devtools::release()
n <- 100
p <- 10
x <- matrix(rnorm(n * p), n, p)
error <- rnorm(n)
y <- 3 * x[, 1] + x[, 2] + error
tau <- 0.5
d = 2
max(n^(-1 / (d + 4)), min(2, sd(y)) * n^(- 1 / (d + 4)))
x[, 1:2]
order(x[, 1:2])
sort(y)
order(y)
which(y == min(y))
0.2*size(n)
0.2*length(y)
0.2*101
floor(0.2*101)
order(y)
order(y)[floor(0.2 * length(y)):(n - 0.2 * length(y))]
length(order(y)[floor(0.2 * length(y)):(n - 0.2 * length(y))])
library(quantdr)
index_y <- order(y)[floor(0.2 * length(y)):(n - 0.2 * length(y))]
index_y
n <- length(y)
p <- dim(x)[2]
# standardize the predictor variables
xc <- scale(x, scale = FALSE)
sig <- var(x)
signrt <- MTS::msqrt(sig)$invsqrt
xstand <- xc %*% signrt
# use SIR for initial dimension reduction
# use bic_d to estimate d, the dimension of the central subspace
output <- dr::dr(y ~ xstand)
lambdas <- output$evalues
d_hat <- bic_d(lambdas, n)
ahat <- cbind(output$evectors[, 1:d_hat])
newx <- xstand %*% ahat
d <- d_hat
d_hat = 1
ahat <- cbind(output$evectors[, 1:d_hat])
newx <- xstand %*% ahat
newx
newx[1:2, ]
index_y <- order(y)[floor(0.2 * length(y)):(n - 0.2 * length(y))]
hm <- dpill(x[index_y, ], y[index_y])
hm <- KernSmooth::dpill(x[index_y, ], y[index_y])
hm
hm*(tau*(1-tau)/(dnorm(qnorm(tau)))^2)^.2
red_dim <- floor(0.2 * n)
red_dim
0.2 * 500
order(y)[red_dim:(n - red_dim)]
max(n^(-1 / (p + 4)), min(2, sd(y)) * n^(- 1 / (p + 4)))
max(n^(-1 / (p + 4)), min(2, sd(y[index_y])) * n^(- 1 / (p + 4)))
n1 <- length(y[index_y])
max(n1^(-1 / (p + 4)), min(2, sd(y[index_y])) * n1^(- 1 / (p + 4)))
devtools::load_all(".")
library(quantdr)
# estimate the directions of a single-index model
set.seed(1234)
n <- 100
p <- 10
x <- matrix(rnorm(n * p), n, p)
error <- rnorm(n)
y <- 3 * x[, 1] + x[, 2] + error
tau <- 0.5
x <- as.matrix(x)
y <- as.matrix(y)
n <- length(y)
p <- dim(x)[2]
# standardize the predictor variables
xc <- scale(x, scale = FALSE)
sig <- var(x)
signrt <- MTS::msqrt(sig)$invsqrt
xstand <- xc %*% signrt
output <- dr::dr(y ~ xstand)
lambdas <- output$evalues
d_hat <- bic_d(lambdas, n)
ahat <- cbind(output$evectors[, 1:d_hat])
newx <- xstand %*% ahat
d <- d_hat
bic_d <- function(lambdas, n) {
lambdas <- sort(lambdas, decreasing = TRUE)
p <- length(lambdas)
gn <- as.null(p)
for (i in 1:length(lambdas)) {
gn[i] <- n * sum((lambdas[1:i])^2) / sum((lambdas)^2) -
2 * (n^ (3 / 4) / p) * i * (i + 1) / 2
}
which(gn == max(gn))
}
output <- dr::dr(y ~ xstand)
lambdas <- output$evalues
d_hat <- bic_d(lambdas, n)
ahat <- cbind(output$evectors[, 1:d_hat])
newx <- xstand %*% ahat
d <- d_hat
d
red_dim <- floor(0.2 * n)
red_dim
index_y <- order(y)[red_dim:(n - red_dim)]
index_y
h <- KernSmooth::dpill(newx[index_y, ], y[index_y])
h <- h * (tau * (1 - tau) / (dnorm(qnorm(tau)))^2)^.2
h
n^(-1 / (d + 4))
min(2, sd(y)) * n^(- 1 / (d + 4))
h
h <- max(n^(-1 / (d + 4)), min(2, sd(y)) * n^(- 1 / (d + 4)), h)
h
non_par <- llqr(newx, y, tau = tau, h = h)
qhat <- non_par$ll_est
# define the initial vector, i.e., the ordinary least squares estimator from
# regressing qhat on x
beta_hat <- (solve(crossprod(cbind(1, xstand))) %*% crossprod(cbind(1, xstand), qhat))[-1]
beta_hat
library(quantdr)
library(PerformanceAnalytics)
data(edhec, package = "PerformanceAnalytics")
head(edhec)
y <- as.vector(edhec[, 2])
n <- length(y)
p <- 5
tau <- 0.05
ValAR(y, p = p, tau = tau)
VaR(y, 0.95, method = 'historical')
size <- 100
VaRest <- as.null(size)
for (i in 1:size){
VaRest[i] <- ValAR(y[1:(n - size + i - 1)], p, tau)
}
y <- y[1:(n - size + i - 1)]
p
newy <- y[-c(1:p)]
n <- length(newy)
# define the design matrix X, which is defined as the previous p observations
# for each return
X <- matrix(0, n, p)
for (i in (p + 1):(n + p)){
X[i - p, ] <- y[(i - p):(i - 1)]
}
movwind <- min(250, length(newy))
newy <- newy[(n - movwind + 1):n]
X <- X[(n - movwind + 1):n, ]
n <- length(newy)
n
# one-step ahead prediction
out <- cqs(x = X, y = newy, tau = tau)
z = X; y = newy
x = X; y = newy
dim(x)
length(y)
x <- as.matrix(x)
y <- as.matrix(y)
n <- length(y)
p <- dim(x)[2]
# standardize the predictor variables
xc <- scale(x, scale = FALSE)
sig <- var(x)
signrt <- MTS::msqrt(sig)$invsqrt
xstand <- xc %*% signrt
# use SIR for initial dimension reduction
# use bic_d to estimate d, the dimension of the central subspace
output <- dr::dr(y ~ xstand)
lambdas <- output$evalues
d_hat <- bic_d(lambdas, n)
ahat <- cbind(output$evectors[, 1:d_hat])
newx <- xstand %*% ahat
d <- d_hat
d
red_dim <- floor(0.2 * n)
red_dim
index_y <- order(y)[red_dim:(n - red_dim)]
h <- KernSmooth::dpill(newx[index_y, ], y[index_y])
h <- h * (tau * (1 - tau) / (dnorm(qnorm(tau)))^2)^.2
h
red_dim <- floor(0.2 * n) # find the 20% of the observations
index_y <- order(y)[red_dim:(n - red_dim)] # subtract the smallest 20% and the largest 20% of the observations
h <- KernSmooth::dpill(newx[index_y, ], y[index_y])
h <- h * (tau * (1 - tau) / (dnorm(qnorm(tau)))^2)^.2
if (h == 'NaN') {
h <- max(n^(-1 / (d + 4)), min(2, sd(y)) * n^(- 1 / (d + 4)))
} else {
h <- max(n^(-1 / (d + 4)), min(2, sd(y)) * n^(- 1 / (d + 4)), h) # maximum of all bandwidths
}
h
non_par <- llqr(newx, y, tau = tau, h = h)
devtools::load_all(".")
rm(list = c("bic_d"))
devtools::load_all(".")
size <- 100
VaRest <- as.null(size)
for (i in 1:size){
VaRest[i] <- ValAR(y[1:(n - size + i - 1)], p, tau)
}
y = y[1:(n - size + i - 1)]
i
newy <- y[-c(1:p)]
n <- length(newy)
# define the design matrix X, which is defined as the previous p observations
# for each return
X <- matrix(0, n, p)
for (i in (p + 1):(n + p)){
X[i - p, ] <- y[(i - p):(i - 1)]
}
movwind <- min(250, length(newy))
newy <- newy[(n - movwind + 1):n]
X <- X[(n - movwind + 1):n, ]
n <- length(newy)
x = X
y = newy
bic_d <- function(lambdas, n) {
lambdas <- sort(lambdas, decreasing = TRUE)
p <- length(lambdas)
gn <- as.null(p)
for (i in 1:length(lambdas)) {
gn[i] <- n * sum((lambdas[1:i])^2) / sum((lambdas)^2) -
2 * (n^ (3 / 4) / p) * i * (i + 1) / 2
}
which(gn == max(gn))
}
x <- as.matrix(x)
y <- as.matrix(y)
n <- length(y)
p <- dim(x)[2]
# standardize the predictor variables
xc <- scale(x, scale = FALSE)
sig <- var(x)
signrt <- MTS::msqrt(sig)$invsqrt
xstand <- xc %*% signrt
output <- dr::dr(y ~ xstand)
lambdas <- output$evalues
d_hat <- bic_d(lambdas, n)
ahat <- cbind(output$evectors[, 1:d_hat])
newx <- xstand %*% ahat
d <- d_hat
d
red_dim <- floor(0.2 * n) # find the 20% of the observations
index_y <- order(y)[red_dim:(n - red_dim)] # subtract the smallest 20% and the largest 20% of the observations
h <- KernSmooth::dpill(newx[index_y, ], y[index_y])
h <- h * (tau * (1 - tau) / (dnorm(qnorm(tau)))^2)^.2
h
if (h == 'NaN') {
h <- max(n^(-1 / (d + 4)), min(2, sd(y)) * n^(- 1 / (d + 4)))
} else {
h <- max(n^(-1 / (d + 4)), min(2, sd(y)) * n^(- 1 / (d + 4)), h) # maximum of all bandwidths
}
h
n^(-1 / (d + 4))
min(2, sd(y)) * n^(- 1 / (d + 4))
h
non_par <- llqr(newx, y, tau = tau, h = h)
i
size <- 100
VaRest <- as.null(size)
for (i in 1:size){
VaRest[i] <- ValAR(y[1:(n - size + i - 1)], p, tau)
}
plot.ts(y[(n - size + 1):n], ylim = range(y[(n - size + 1):n], VaRest), ylab = 'returns')
lines(VaRest, col = 'red')
taus <- c(0.01, 0.025, 0.05)
VaRest <- matrix(0, size, length(taus))
for (i in 1:size) {
for (j in 1:length(taus)) {
VaRest[i, j] <- ValAR(y[1:(n - size + i - 1)], p, taus[j])
}
}
# plots
plot.ts(y[(n - size + 1):n], ylim = range(y[(n - size + 1):n], VaRest), ylab = 'returns')
lines(VaRest[, 1], col = 'red')
lines(VaRest[, 2], col = 'blue')
lines(VaRest[, 3], col = 'green')
legend('top', legend = c("1%", "2.5%", "5%"), col = c("red", "blue", "green"),
lty=1, cex=1, horiz = T, bty = "n")
# proportion of exceptions
sum(y[(n - size + 1):n] < VaRest[, 1]) / size
sum(y[(n - size + 1):n] < VaRest[, 2]) / size
sum(y[(n - size + 1):n] < VaRest[, 3]) / size
y = y[1:(n - size + i - 1)]
newy <- y[-c(1:p)]
n <- length(newy)
# define the design matrix X, which is defined as the previous p observations
# for each return
X <- matrix(0, n, p)
for (i in (p + 1):(n + p)){
X[i - p, ] <- y[(i - p):(i - 1)]
}
movwind <- min(250, length(newy))
newy <- newy[(n - movwind + 1):n]
X <- X[(n - movwind + 1):n, ]
n <- length(newy)
x = X
y = newy
tau = tau
tau
x <- as.matrix(x)
y <- as.matrix(y)
n <- length(y)
p <- dim(x)[2]
# standardize the predictor variables
xc <- scale(x, scale = FALSE)
sig <- var(x)
signrt <- MTS::msqrt(sig)$invsqrt
xstand <- xc %*% signrt
output <- dr::dr(y ~ xstand)
lambdas <- output$evalues
d_hat = 1
ahat <- cbind(output$evectors[, 1:d_hat])
newx <- xstand %*% ahat
red_dim <- floor(0.2 * n) # find the 20% of the observations
index_y <- order(y)[red_dim:(n - red_dim)] # subtract the smallest 20% and the largest 20% of the observations
h <- KernSmooth::dpill(newx[index_y, ], y[index_y])
h <- h * (tau * (1 - tau) / (dnorm(qnorm(tau)))^2)^.2
if (h == 'NaN') {
h <- 2 * max(n^(-1 / (d + 4)), min(2, sd(y)) * n^(- 1 / (d + 4)))
} else {
h <- 2 * max(n^(-1 / (d + 4)), min(2, sd(y)) * n^(- 1 / (d + 4)), h) # maximum of all bandwidths
}
h
max(n^(-1 / (d + 4)), min(2, sd(y)) * n^(- 1 / (d + 4)))
max(n^(-1 / (d + 4)), min(2, sd(y)) * n^(- 1 / (d + 4)), h)
red_dim <- floor(0.2 * n) # find the 20% of the observations
index_y <- order(y)[red_dim:(n - red_dim)] # subtract the smallest 20% and the largest 20% of the observations
h <- KernSmooth::dpill(newx[index_y, ], y[index_y])
h <- h * (tau * (1 - tau) / (dnorm(qnorm(tau)))^2)^.2
max(n^(-1 / (d + 4)), min(2, sd(y)) * n^(- 1 / (d + 4)))
max(n^(-1 / (d + 4)), min(2, sd(y)) * n^(- 1 / (d + 4)), h)
2 * max(n^(-1 / (d + 4)), min(2, sd(y)) * n^(- 1 / (d + 4)), h)
1.5 * max(n^(-1 / (d + 4)), min(2, sd(y)) * n^(- 1 / (d + 4)), h)
1.25 * max(n^(-1 / (d + 4)), min(2, sd(y)) * n^(- 1 / (d + 4)), h)
devtools::load_all(".")
rm(list = c("bic_d"))
devtools::load_all(".")
taus <- c(0.01, 0.025, 0.05)
VaRest <- matrix(0, size, length(taus))
for (i in 1:size) {
for (j in 1:length(taus)) {
VaRest[i, j] <- ValAR(y[1:(n - size + i - 1)], p, taus[j])
}
}
# plots
plot.ts(y[(n - size + 1):n], ylim = range(y[(n - size + 1):n], VaRest), ylab = 'returns')
lines(VaRest[, 1], col = 'red')
lines(VaRest[, 2], col = 'blue')
lines(VaRest[, 3], col = 'green')
legend('top', legend = c("1%", "2.5%", "5%"), col = c("red", "blue", "green"),
lty=1, cex=1, horiz = T, bty = "n")
# proportion of exceptions
sum(y[(n - size + 1):n] < VaRest[, 1]) / size
sum(y[(n - size + 1):n] < VaRest[, 2]) / size
sum(y[(n - size + 1):n] < VaRest[, 3]) / size
library(PerformanceAnalytics)
data(edhec, package = "PerformanceAnalytics")
head(edhec)
y <- as.vector(edhec[, 2])
n <- length(y)
p <- 5
tau <- 0.05
ValAR(y, p = p, tau = tau)
VaR(y, 0.95, method = 'historical')
size <- 100
VaRest <- as.null(size)
for (i in 1:size){
VaRest[i] <- ValAR(y[1:(n - size + i - 1)], p, tau)
}
sum(y[(n - size + 1):n] < VaRest) / size
taus <- c(0.01, 0.025, 0.05)
VaRest <- matrix(0, size, length(taus))
for (i in 1:size) {
for (j in 1:length(taus)) {
VaRest[i, j] <- ValAR(y[1:(n - size + i - 1)], p, taus[j])
}
}
# plots
plot.ts(y[(n - size + 1):n], ylim = range(y[(n - size + 1):n], VaRest), ylab = 'returns')
lines(VaRest[, 1], col = 'red')
lines(VaRest[, 2], col = 'blue')
lines(VaRest[, 3], col = 'green')
legend('top', legend = c("1%", "2.5%", "5%"), col = c("red", "blue", "green"),
lty=1, cex=1, horiz = T, bty = "n")
# proportion of exceptions
sum(y[(n - size + 1):n] < VaRest[, 1]) / size
sum(y[(n - size + 1):n] < VaRest[, 2]) / size
sum(y[(n - size + 1):n] < VaRest[, 3]) / size
devtools::load_all(".")
library(PerformanceAnalytics)
data(edhec, package = "PerformanceAnalytics")
head(edhec)
y <- as.vector(edhec[, 2])
n <- length(y)
p <- 5
tau <- 0.05
ValAR(y, p = p, tau = tau)
VaR(y, 0.95, method = 'historical')
size <- 100
VaRest <- as.null(size)
for (i in 1:size){
VaRest[i] <- ValAR(y[1:(n - size + i - 1)], p, tau)
}
plot.ts(y[(n - size + 1):n], ylim = range(y[(n - size + 1):n], VaRest), ylab = 'returns')
lines(VaRest, col = 'red')
sum(y[(n - size + 1):n] < VaRest) / size
taus <- c(0.01, 0.025, 0.05)
VaRest <- matrix(0, size, length(taus))
for (i in 1:size) {
for (j in 1:length(taus)) {
VaRest[i, j] <- ValAR(y[1:(n - size + i - 1)], p, taus[j])
}
}
devtools::load_all(".")
library(quantdr)
library(quantdr)
library(PerformanceAnalytics)
data(edhec, package = "PerformanceAnalytics")
head(edhec)
y <- as.vector(edhec[, 2])
n <- length(y)
p <- 5
tau <- 0.05
ValAR(y, p = p, tau = tau)
VaR(y, 0.95, method = 'historical')
size <- 100
VaRest <- as.null(size)
for (i in 1:size){
VaRest[i] <- ValAR(y[1:(n - size + i - 1)], p, tau)
}
plot.ts(y[(n - size + 1):n], ylim = range(y[(n - size + 1):n], VaRest), ylab = 'returns')
lines(VaRest, col = 'red')
sum(y[(n - size + 1):n] < VaRest) / size
taus <- c(0.01, 0.025, 0.05)
VaRest <- matrix(0, size, length(taus))
for (i in 1:size) {
for (j in 1:length(taus)) {
VaRest[i, j] <- ValAR(y[1:(n - size + i - 1)], p, taus[j])
}
}
n <- 100
p <- 10
x <- matrix(rnorm(n * p), n, p)
error <- rnorm(n)
y <- 3 * x[, 1] + x[, 2] + error
# plots
plot.ts(y[(n - size + 1):n], ylim = range(y[(n - size + 1):n], VaRest), ylab = 'returns')
lines(VaRest[, 1], col = 'red')
lines(VaRest[, 2], col = 'blue')
lines(VaRest[, 3], col = 'green')
legend('top', legend = c("1%", "2.5%", "5%"), col = c("red", "blue", "green"),
lty=1, cex=1, horiz = T, bty = "n")
# proportion of exceptions
sum(y[(n - size + 1):n] < VaRest[, 1]) / size
sum(y[(n - size + 1):n] < VaRest[, 2]) / size
sum(y[(n - size + 1):n] < VaRest[, 3]) / size
tau
taus
devtools::load_all(".")
library(PerformanceAnalytics)
data(edhec, package = "PerformanceAnalytics")
head(edhec)
y <- as.vector(edhec[, 2])
n <- length(y)
p <- 5
tau <- 0.05
ValAR(y, p = p, tau = tau)
VaR(y, 0.95, method = 'historical')
taus <- c(0.01, 0.025, 0.05)
VaRest <- matrix(0, size, length(taus))
for (i in 1:size) {
for (j in 1:length(taus)) {
VaRest[i, j] <- ValAR(y[1:(n - size + i - 1)], p, taus[j])
}
}
# plots
plot.ts(y[(n - size + 1):n], ylim = range(y[(n - size + 1):n], VaRest), ylab = 'returns')
lines(VaRest[, 1], col = 'red')
lines(VaRest[, 2], col = 'blue')
lines(VaRest[, 3], col = 'green')
legend('top', legend = c("1%", "2.5%", "5%"), col = c("red", "blue", "green"),
lty=1, cex=1, horiz = T, bty = "n")
# proportion of exceptions
sum(y[(n - size + 1):n] < VaRest[, 1]) / size
sum(y[(n - size + 1):n] < VaRest[, 2]) / size
sum(y[(n - size + 1):n] < VaRest[, 3]) / size
devtools::check_win_devel()
devtools::check_rhub()
devtools::release()
devtools::spell_check()
devtools::release()
